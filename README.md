# [ICASSP 2026] INTER-DIALOG CONTRASTIVE LEARNING FOR MULTIMODAL EMOTION RECOGNITION IN CONVERSATIONS

<div align="center">

**Dong-Hyuk Lee**<sup>1</sup>, **Dae Hyeon Kim**<sup>1</sup>, **Young-Seok Choi**<sup>1*</sup>

<sup>1</sup>Department of Electronics and Communications Engineering, Kwangwoon University, Seoul, South Korea

(*Corresponding author)

[![Conference](https://img.shields.io/badge/ICASSP-2026-4b44ce.svg)](https://2026.ieeeicassp.org/)

</div>

---

## üì¢ News
* **[Jan, 2026]** üéâ Our paper **"INTER-DIALOG CONTRASTIVE LEARNING FOR MULTIMODAL EMOTION RECOGNITION IN CONVERSATIONS"** has been accepted to **ICASSP 2026**!
* **[Coming Soon]** üöÄ The official code and pre-trained models for **IDCL** will be released soon.

---

## üìù Abstract
Multimodal Emotion Recognition in Conversations (MERC) is challenging due to the complex interplay between modalities and the critical role of contextual information. While previous studies have primarily focused on context within a single conversation (intra-dialog), this work explores a new dimension: the contextual information shared across different conversations. 

We introduce **Inter-Dialog Contrastive Learning (IDCL)**, a novel framework that leverages inter-dialog similarities to enhance multimodal representation learning. IDCL operates on the hypothesis that conversations with similar emotional trajectories share underlying contextual patterns. By maximizing the similarity between these emotionally congruent dialogs and minimizing it for incongruent ones, IDCL learns more robust and generalizable representations. 

Experiments on the **IEMOCAP** dataset demonstrate that our approach significantly outperforms state-of-the-art methods, establishing the importance of inter-dialog context for advancing emotion recognition.

*(We recommend adding Figure 1 from the paper here)*


## üóìÔ∏è To-Do List
We are currently organizing the code for release.
- [ ] Add citation form & key results
- [ ] Upload data preprocessing scripts (IEMOCAP)
- [ ] Upload IDCL training code
- [ ] Share pre-trained weights

---
